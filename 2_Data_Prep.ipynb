{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, platform\n",
    "\n",
    "import re\n",
    "from pprint import pprint \n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "Establish my credentials for the Spotify API, and setup up an object to use for calls to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_id = 'c5c5978ddbf94927a493aaa72f9d197a'\n",
    "spotify_secret = '6e531cfaa4134f1a9269cc4c0a364b1d'\n",
    "REQUEST_TIMEOUT = 4\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id = spotify_id, \n",
    "    client_secret = spotify_secret)\n",
    "sp = spotipy.Spotify(\n",
    "    client_credentials_manager = client_credentials_manager, \n",
    "    requests_timeout = REQUEST_TIMEOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore\n",
    "\n",
    "Last.FM data downloaded via https://benjaminbenben.com/lastfm-to-csv/.\n",
    "\n",
    "Load the raw data. Notice how there are some missing album titles and timestamps. This is likely just the result of a bad script pulling from Last.FM, so we'll have to fix that. Below the counts is a random sample of the data, just to get a feel of what is in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_history = pd.read_csv('data/alexliebscher.csv')\n",
    "print(original_history.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing missing timestamp data won't be hard, we will just backfill to take care of that. A very small percentage is missing, and I assume the missing values have a high probability of being similar to the song before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_history['timestamp'] = original_history['timestamp'].bfill()\n",
    "print(original_history['timestamp'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamps are also missing timestamp information, so we should add that to ensure our analysis reflects my local time. In this case, all timestamps are assumed to be UTC and are converted to US/Pacific, my local zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctTimestamp(timestamp):\n",
    "    '''\n",
    "    Correct missing timezone information to US/Pacific from UTC\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamp : str, pandas.Timestamp\n",
    "        The naive timestamp to correct\n",
    "        \n",
    "    Return\n",
    "    ----------\n",
    "    A corrected, US/Pacific aware timestamp\n",
    "    '''\n",
    "    if type(timestamp) is str:\n",
    "        timestamp = Timestamp(datetime.strptime(timestamp, '%x %H:%M').replace(tzinfo=timezone('UTC')))\n",
    "    if timestamp.tzinfo is None:\n",
    "        timestamp = timestamp.tz_localize('UTC')\n",
    "        \n",
    "    return timestamp.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezoned_history = original_history\n",
    "timezoned_history['timestamp'] = timezoned_history['timestamp'].apply(correctTimestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first song was recorded on December 18th, 2017 at roughly 7pm. This dataset covers the following 101 days after that. A random sample is available to see the corrected timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_max = timezoned_history['timestamp'].max()\n",
    "history_min = timezoned_history['timestamp'].min()\n",
    "\n",
    "print(history_min)\n",
    "print(history_max - history_min)\n",
    "timezoned_history.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch full track data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimeter_pattern = re.compile(\"[\\{\\}\\[\\]\\(\\)\\#\\'\\\"]\")\n",
    "classical_pattern = re.compile(\"((op\\.?|no\\.?)\\s*\\d{1,3}\\s?)\", re.IGNORECASE)\n",
    "collections_pattern = re.compile(\"(^\\d{1,3}\\s*)\")\n",
    "stylizations_pattern = re.compile(\"[\\,\\-\\_\\&\\*]\\s?|\\:\\s\")\n",
    "\n",
    "def get_track_info(track, artist, album='', id_excl=False):\n",
    "    '''\n",
    "    With a track name and artist, and optionally an album name,\n",
    "    search for a corresponding track via the Spotify API and\n",
    "    build an object with possible descriptive data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    track : str\n",
    "        The name of a track\n",
    "    artist : str\n",
    "        The name of the track's artist\n",
    "    album : str, optional\n",
    "        The name of the track's album\n",
    "    id_excl : bool, optional\n",
    "        Return only the track's Spotify ID\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    Descriptive track data, or just the track ID, or an empty\n",
    "    dict if no data could be found for the specified track\n",
    "    '''\n",
    "        # remove (feat. some artist) for cleaner search\n",
    "    track = track.lower()\n",
    "    if \" (feat\" in track:\n",
    "        track = track.split(\" (feat\")[0]\n",
    "    elif \" (with\" in track:\n",
    "        track = track.split(\" (with\")[0]\n",
    "    elif \" (&\" in track:\n",
    "        track = track.split(\" (&\")[0]\n",
    "        \n",
    "    # clean album names too\n",
    "    album = album.lower()\n",
    "    if \"nan\" == album:\n",
    "        album = \"\"\n",
    "    elif \" (feat\" in album:\n",
    "        album = album.split(\" (feat\")[0]\n",
    "    elif \" (with\" in album:\n",
    "        album = album.split(\" (with\")[0]\n",
    "    elif \" (&\" in album:\n",
    "        album = album.split(\" (&\")[0]\n",
    "        \n",
    "    # compose a clean, simple query string\n",
    "    query = str(track + ' ' + artist + ' ' + album).strip()\n",
    "    \n",
    "    query = delimeter_pattern.sub(\"\", query) # remove various delimeter chars\n",
    "    query, subs = classical_pattern.subn(\"\", query) # remove common strings in classical track titles\n",
    "                                                    # unfortunately modifies tracks such as Candy Shop \n",
    "                                                    # by 50 Cent to \"candy shCent\"\n",
    "    if subs > 0:\n",
    "        # classical music often starts with the number of pieces in\n",
    "        # a collection (\"12 Etudes, Op. 10: No.10 in C minor\")\n",
    "        query = collections_pattern.sub(\"\", query)\n",
    "        \n",
    "    query = stylizations_pattern.sub(\" \", query) # common stylizations in track/album names\n",
    "        \n",
    "    # store a new track\n",
    "    _track = {}\n",
    "    \n",
    "    # if the song exists in the Spotify catalog, fetch info\n",
    "    try:\n",
    "        meta = sp.search(q='track:' + query, type='track', limit=1)\n",
    "        meta = meta['tracks']['items'][0]\n",
    "\n",
    "        if not id_excl:\n",
    "            features = sp.audio_features([meta['id']])[0]\n",
    "            \n",
    "    except Exception as e:\n",
    "        # if the track could not be found, try once more without the album\n",
    "        if album is not \"\":\n",
    "            \n",
    "            retry = get_track_info(track, artist)\n",
    "            # if the track couldn't be found without the album, give up\n",
    "            if retry:\n",
    "                return retry\n",
    "            \n",
    "        print('no data for: {} by {} ({})'.format(track, artist, album))\n",
    "        print('query: {}\\n'.format(query))\n",
    "        return {}\n",
    "\n",
    "    if id_excl and meta['id']:\n",
    "        return meta['id']\n",
    "    \n",
    "    # store relevant information and return the object\n",
    "    try:\n",
    "        _track['id'] = meta['id']\n",
    "        _track['name'] = meta['name']\n",
    "        _track['release'] = meta['album']['release_date']\n",
    "        _track['popularity'] = meta['popularity']\n",
    "        _track['explicit'] = int(meta['explicit'])\n",
    "        _track['artists'] = [a['id'] for a in meta['artists']]\n",
    "        _track['album'] = meta['album']['name']\n",
    "\n",
    "        _track['acousticness'] = features['acousticness']\n",
    "        _track['danceability'] = features['danceability']\n",
    "        _track['duration_ms'] = features['duration_ms']\n",
    "        _track['energy'] = features['energy']\n",
    "        _track['key'] = features['key']\n",
    "        _track['liveness'] = features['liveness']\n",
    "        _track['loudness'] = features['loudness']\n",
    "        _track['mode'] = features['mode']\n",
    "        _track['speechiness'] = features['speechiness']\n",
    "        _track['tempo'] = features['tempo']\n",
    "        _track['time_signature'] = features['time_signature']\n",
    "        _track['valence'] = features['valence']\n",
    "    except TypeError:\n",
    "        return {}\n",
    "    \n",
    "    return _track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a random sample of 50 tracks. I can use this to compare single processor efficiency with multiprocessor efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = timezoned_history.sample(50)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline multiprocessor efficiency\n",
    "\n",
    "Time and record serial processing and then time multiprocessor functionality. Let this be a simple measurement of how well we can do with multiprocessing when fetching track data from the API. I learned that this was the way to go, especially as fetch 5,000+ tracks multiple times during testing, and as I update my notebook. Unfortuntaly, when beginning this project, I ran `get_track_info()` over 7,400 times with serial processing and that took about an hour and 58 minutes. Never again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = mp.cpu_count() # used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a sharedctypes integer to count records\n",
    "v = mp.Value('i', 0, lock=False)\n",
    "\n",
    "def async_fetch(track, artist, album):\n",
    "    '''\n",
    "    Count and display track searches and timing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    track : str\n",
    "        The name of a track\n",
    "    artist : str\n",
    "        The name of the track's artist\n",
    "    album : str\n",
    "        The name of the track's album\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    Data about the track, if the track is found (otherwise, empty dict)\n",
    "    '''\n",
    "    if v.value % 10 == 0 and v.value is not 0:\n",
    "        # after every 10 tracks searched, print progress information\n",
    "        print('record: #{} at ({})\\n'.format(str(v.value), datetime.now() - s))\n",
    "        \n",
    "    v.value += 1\n",
    "    return get_track_info(track, artist, album)\n",
    "\n",
    "def serial(tracks):\n",
    "    '''\n",
    "    A serial processor for comparison's purpose (1 CPU, 1 process)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tracks : list\n",
    "        A list of tracks to search\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    A list of track data in dicts\n",
    "    '''\n",
    "    return [get_track_info(str(t['track']), str(t['artist']), str(t['album'])) for i, t in tracks.iterrows()]\n",
    "\n",
    "def multiprocess(processes, tracks):  \n",
    "    '''\n",
    "    Multiprocessing to utilize all cores for comparison's purpose\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    processes : int\n",
    "        The number of processes to create in parallel\n",
    "    tracks : list\n",
    "        A list of tracks to search\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    A list of track data in dicts\n",
    "    '''\n",
    "    pool = mp.Pool(processes=processes)\n",
    "    results = [pool.apply_async(async_fetch, args=(str(t['track']), str(t['artist']), str(t['album']))) for i, t in tracks.iterrows()]\n",
    "    results = [p.get() for p in results]\n",
    "        \n",
    "    return results\n",
    "\n",
    "print('\\n')\n",
    "print('# of CPUs:\\t{}'.format(cpus))\n",
    "print('Python version:\\t{}'.format(platform.python_version()))\n",
    "print('Compiler:\\t{}'.format(platform.python_compiler()))\n",
    "print('System:\\t\\t{}'.format(platform.system()))\n",
    "print('Release:\\t{}'.format(platform.release()))\n",
    "print('Machine:\\t{}'.format(platform.machine()))\n",
    "print('Processor:\\t{}'.format(platform.processor()))\n",
    "print('Interpreter:\\t{}'.format(platform.architecture()[0]))\n",
    "print('\\n')\n",
    "\n",
    "# Test and time serial()\n",
    "s = datetime.now()\n",
    "serial_temp = pd.DataFrame(serial(sample)).dropna()\n",
    "serial_t = datetime.now() - s\n",
    "\n",
    "# Test and time multiprocess()\n",
    "s = datetime.now()\n",
    "multi_temp = pd.DataFrame(multiprocess(cpus, sample)).dropna()\n",
    "multi_t = datetime.now() - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Serial Processing')\n",
    "print('search ratio (found : expected): {}'.format(len(serial_temp)/len(sample)))\n",
    "print('Time: {}'.format(serial_t))\n",
    "print('\\nMulti Processing')\n",
    "print('search ratio (found : expected): {}'.format(len(multi_temp)/len(sample)))\n",
    "print('Time: {}'.format(multi_t))\n",
    "print('\\n{0:.2f}x faster with multiprocess'.format(serial_t / multi_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch all track data from Spotify\n",
    "\n",
    "Use all 4 of my CPUs to fetch track information in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sharedctype integers to count records\n",
    "v = mp.Value('i', 0, lock=False)\n",
    "total = None\n",
    "\n",
    "def async_fetch_real(track, artist, album, timestamp):\n",
    "    '''\n",
    "    Count and display track searches and timing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    track : str\n",
    "        The name of a track\n",
    "    artist : str\n",
    "        The name of the track's artist\n",
    "    album : str\n",
    "        The name of the track's album\n",
    "    timestamp : str\n",
    "        The timestamp of the track\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    Data about the track, if the track is found (otherwise, empty dict)    \n",
    "    '''\n",
    "    if v.value % 100 == 0 and v.value is not 0:\n",
    "        # after every 100 tracks searched, print progress information\n",
    "        elap = datetime.now() - s\n",
    "        print('record: #{} - remaining: {}\\n'.format(str(v.value), ((elap/v.value) * total.value) - elap))\n",
    "        \n",
    "    v.value += 1\n",
    "    \n",
    "    _t = get_track_info(track, artist, album)\n",
    "    # re-attach the timestamp to the track data\n",
    "    _t.update({\n",
    "        'timestamp': timestamp\n",
    "    })\n",
    "        \n",
    "    return _t\n",
    "\n",
    "def multiprocess(processes, tracks):\n",
    "    '''\n",
    "    Multiprocessing to utilize all cores for all listening history\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    processes : int\n",
    "        The number of processes to create in parallel\n",
    "    tracks : list\n",
    "        A list of tracks to search\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    A list of track data in dicts    \n",
    "    '''\n",
    "    pool = mp.Pool(processes=processes)\n",
    "    results = [pool.apply_async(async_fetch_real, args=(str(t['track']), str(t['artist']), str(t['album']), str(t['timestamp']),)) for i, t in tracks.iterrows()]\n",
    "    results = [p.get() for p in results]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def updateTracks(original, unique):\n",
    "    '''\n",
    "    Fetch track data via the Spotify API and save the compiled output to JSON\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    original : pandas.DataFrame\n",
    "    unique : pandas.DataFrame\n",
    "    '''\n",
    "    temp = pd.DataFrame(multiprocess(cpus, unique)).dropna()\n",
    "    multi_t = datetime.now() - s\n",
    "    \n",
    "    if not temp.empty:\n",
    "        compiled = pd.concat([original, temp], ignore_index=True)\n",
    "        compiled.to_json('data/history_comp.json')\n",
    "        print('Saved complete history')\n",
    "        \n",
    "        print('Search ratio (found : expected): {}'.format(len(temp)/total.value))\n",
    "        print('Total time:\\t\\t\\t {}'.format(multi_t))\n",
    "        print('Songs/sec fetched:\\t\\t {}'.format(total.value/multi_t.total_seconds()))\n",
    "        \n",
    "        return compiled\n",
    "    else:\n",
    "        print('No new track data found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_full_history = pd.read_json('data/history_comp.json') if os.path.isfile('data/history_comp.json') else pd.DataFrame(columns=['timestamp'])\n",
    "last_full_history['timestamp'] = last_full_history['timestamp'].apply(correctTimestamp)\n",
    "\n",
    "s = datetime.now()\n",
    "\n",
    "if last_full_history.empty or last_full_history['timestamp'].max() < timezoned_history['timestamp'].max():\n",
    "    unique = timezoned_history[~timezoned_history['timestamp'].isin(last_full_history['timestamp'])]\n",
    "    \n",
    "    total = mp.Value('i', len(unique), lock=False)\n",
    "        \n",
    "    updateTracks(last_full_history, unique)\n",
    "else:\n",
    "    print('all records up to date')\n",
    "    \n",
    "# record: #9200 - remaining: 0:00:07.263164\n",
    "\n",
    "# Saved complete history\n",
    "# Search ratio (found : expected): 0.9913419913419913\n",
    "# Total time:\t\t\t 0:27:58.994141\n",
    "# Songs/sec fetched:\t\t 5.5032949635528245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_json('data/artist_info.json')\n",
    "artists.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history = pd.read_json('data/history_comp.json')\n",
    "full_history['timestamp'] = full_history['timestamp'].apply(correctTimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_artist_ids = []\n",
    "\n",
    "for i, r in full_history.iterrows():\n",
    "    for artist in r['artists']:\n",
    "        if not (artists['id'] == artist).any():\n",
    "            missing_artist_ids.append(artist)\n",
    "            \n",
    "len(missing_artist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_info(id):\n",
    "    try:\n",
    "        result = sp.artist(id)\n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    return {'artist': result['name'],\n",
    "            'id': result['id'], \n",
    "            'genres': np.array(result['genres']), \n",
    "            'popularity': result['popularity'], \n",
    "            'followers': result['followers']['total']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a sharedctypes integer to count records\n",
    "v = mp.Value('i', 0, lock=False)\n",
    "total = mp.Value('i', len(missing_artist_ids), lock=False)\n",
    "\n",
    "cpus = mp.cpu_count()\n",
    "\n",
    "def async_fetch_real(id):\n",
    "    '''\n",
    "    count and display artist searches and timing\n",
    "    '''\n",
    "    if v.value % 100 == 0 and v.value is not 0:\n",
    "        print('record: #{}'.format(v.value))\n",
    "        elap = datetime.now() - s\n",
    "        print('time remaining: {}'.format(((elap/v.value) * total.value) - elap))\n",
    "        \n",
    "    v.value += 1\n",
    "    \n",
    "    return get_artist_info(id)\n",
    "\n",
    "def multiprocess(processes, ids):\n",
    "    '''\n",
    "    multiprocessing to utilize all cores\n",
    "    '''\n",
    "    pool = mp.Pool(processes=processes)\n",
    "    results = [pool.apply_async(async_fetch_real, args=(str(i),)) for i in ids]\n",
    "    results = [p.get() for p in results]\n",
    "    return results\n",
    "\n",
    "s = datetime.now()\n",
    "artists_temp = pd.DataFrame(multiprocess(cpus, missing_artist_ids)).dropna()\n",
    "multi_t = datetime.now() - s\n",
    "\n",
    "stitched_artists = artists.append(artists_temp, ignore_index=True)\n",
    "stitched_artists = stitched_artists.drop_duplicates('id')\n",
    "\n",
    "if not artists_temp.empty:\n",
    "    stitched_artists.to_json('data/artist_info.json'.format(s.month, s.day))\n",
    "    print('Saved artist info')\n",
    "    \n",
    "print('{} artists added (expected {})'.format(len(stitched_artists)-len(artists), len(artists_temp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service to export Last.FM data overcounts the most recently listened to song, so I choose to keep the first quarter of instances and drop the remainder. This prevents an artificial skewing toward a song that shouldn't be the mode of the data set. Drawback: if the first song _really_ is the mode of the dataset, I unknowingly change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wizard Of Finance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3193,\n",
       " 3392,\n",
       " 3592,\n",
       " 3790,\n",
       " 397,\n",
       " 3990,\n",
       " 4188,\n",
       " 4389,\n",
       " 4587,\n",
       " 4787,\n",
       " 4988,\n",
       " 5186,\n",
       " 5386,\n",
       " 5586,\n",
       " 5786,\n",
       " 598,\n",
       " 5986,\n",
       " 6185,\n",
       " 6384,\n",
       " 6582,\n",
       " 6782,\n",
       " 6978,\n",
       " 7177,\n",
       " 7377,\n",
       " 7576,\n",
       " 7775,\n",
       " 797,\n",
       " 7975,\n",
       " 8173,\n",
       " 8368,\n",
       " 8566,\n",
       " 8766,\n",
       " 8964,\n",
       " 997]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_mode = full_history['id'].mode()[0]\n",
    "L = list(full_history.loc[full_history['id'] == track_mode].index)\n",
    "L = L[int(len(L)*0.25):]\n",
    "print(full_history.loc[L[0]]['name'])\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history = full_history.drop(index=L).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history.to_json('data/history_comp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
